{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47XBsjxqb2iB",
        "outputId": "ac0fb6b0-ab41-405d-d551-a0b100b668c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define the input and output directories\n",
        "input_dir = '/content/gdrive/MyDrive/dataset'\n",
        "output_dir = '/content/gdrive/MyDrive/out'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Define the morphological kernel size\n",
        "kernel_size = 10\n",
        "\n",
        "# Process each image in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    # Load the input image\n",
        "    img = cv2.imread(os.path.join(input_dir, filename))\n",
        "    img =cv2.resize(img,(300,300))\n",
        "\n",
        "    # Convert the color image to grayscale using the weighted process\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.addWeighted(gray, 0.3, gray, 0.59, 0)\n",
        "    gray = cv2.addWeighted(gray, 1, gray, 0.11, 0)\n",
        "\n",
        "    # Apply morphological black-hat transformation to identify hair contour\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
        "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n",
        "\n",
        "    # Threshold the image to create a binary mask\n",
        "    _, mask = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "\n",
        "    # Use the Fast-Marching Method to inpaint the hair regions\n",
        "    mask = cv2.inpaint(img, mask, 3, cv2.INPAINT_TELEA)\n",
        "\n",
        "    # Save the result to disk\n",
        "    cv2.imwrite(os.path.join(output_dir, filename), mask)\n",
        "\n",
        "\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Set the directory containing the input images\n",
        "input_dir = '/content/gdrive/MyDrive/out'\n",
        "\n",
        "# Set the directory to save the output images\n",
        "output_dir = '/content/gdrive/MyDrive/blurremoved'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Set the kernel size for the Gaussian blur filter\n",
        "kernel_size = (5,5)\n",
        "\n",
        "# Set the parameters for the unsharp masking filter\n",
        "unsharp_weight = 1.5\n",
        "blur_weight = -0.5\n",
        "\n",
        "# Loop over each image in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    # Load the image\n",
        "    img = cv2.imread(os.path.join(input_dir, filename))\n",
        "\n",
        "    # Apply a Gaussian blur filter\n",
        "    blur = cv2.GaussianBlur(img, kernel_size, 0)\n",
        "\n",
        "    # Apply an unsharp masking filter\n",
        "    unsharp_mask = cv2.addWeighted(img, unsharp_weight, blur, blur_weight, 0)\n",
        "\n",
        "    # Save the output image\n",
        "    cv2.imwrite(os.path.join(output_dir, filename), unsharp_mask)"
      ],
      "metadata": {
        "id": "7LpSG_fKb4_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# Define input and output directories\n",
        "input_dir = '/content/gdrive/MyDrive/blurremoved'\n",
        "output_dir = '/content/gdrive/MyDrive/grabcut'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "for filename in os.listdir(input_dir):\n",
        "    # Read the image file\n",
        "    image = cv2.imread(os.path.join(input_dir, filename))\n",
        "\n",
        "\n",
        "\n",
        "    # Convert the image to HSV color space\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Apply adaptive histogram equalization\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    hsv[:,:,2] = clahe.apply(hsv[:,:,2])\n",
        "\n",
        "    # Convert the image back to BGR color space\n",
        "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    # Apply Grab-cut segmentation\n",
        "    mask = np.zeros(image.shape[:2],np.uint8)\n",
        "    bgdModel = np.zeros((1,65),np.float64)\n",
        "    fgdModel = np.zeros((1,65),np.float64)\n",
        "    rect = (10,12,250,270)\n",
        "    cv2.grabCut(bgr,mask,rect,bgdModel,fgdModel,30,cv2.GC_INIT_WITH_RECT)\n",
        "    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
        "    segmented = bgr*mask2[:,:,np.newaxis]\n",
        "\n",
        "    # Apply a threshold of 1 to the mask\n",
        "    mask2[mask2 ==  1] = 255\n",
        "    mask2[mask2 == 0] = 0\n",
        "\n",
        "    # Save the segmented image to disk\n",
        "    output_filename = os.path.splitext(filename)[0] + '_seg.png'\n",
        "    cv2.imwrite(os.path.join(output_dir, output_filename),segmented)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekf2l8FDb5Zd",
        "outputId": "519c1d45-253f-4728-d77a-216e064036ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "input_dire='/content/gdrive/MyDrive/dataset'\n",
        "input_dir = '/content/gdrive/MyDrive/grabcut'\n",
        "output_dir = '/content/gdrive/MyDrive/originalshape'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "for filename in os.listdir(input_dire):\n",
        "    # Load the input image\n",
        "    img = cv2.imread(os.path.join(input_dire, filename))\n",
        "    new_width=img.shape[0]\n",
        "    new_height=img.shape[1]\n",
        "    # Loop through each group of four images\n",
        "    for filename2 in os.listdir(input_dir):\n",
        "        filename1=filename2[0:12]+\".JPG\"\n",
        "        if(filename==filename1):\n",
        "            img1= cv2.imread(os.path.join(input_dir, filename2))\n",
        "            out=cv2.resize(img1,(new_height,new_width))\n",
        "            cv2.imwrite(os.path.join(output_dir, filename2), out)"
      ],
      "metadata": {
        "id": "Tim8U6Fyb6Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define the directory containing the pre-processed skin images\n",
        "input_dir = \"/content/gdrive/MyDrive/originalshape\"\n",
        "\n",
        "# Define the output directory where augmented images will be saved\n",
        "output_dir = \"/content/gdrive/MyDrive/augmentation\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Define the rotation angles\n",
        "angles = [0, 90, 180, 270]\n",
        "\n",
        "# Loop over all images in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    # Load the image\n",
        "    image = Image.open(os.path.join(input_dir, filename))\n",
        "\n",
        "    # Loop over the rotation angles\n",
        "    for angle in angles:\n",
        "        # Rotate the image\n",
        "        rotated_image = image.rotate(angle)\n",
        "\n",
        "        # Generate a new filename for the rotated image\n",
        "        new_filename = filename[:-4] + \"_\" + str(angle) + \".jpg\"\n",
        "\n",
        "        # Save the rotated image to the output directory\n",
        "        rotated_image.save(os.path.join(output_dir, new_filename))"
      ],
      "metadata": {
        "id": "8G98aQkpcHS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "a=[]\n",
        "b=[]\n",
        "c=[]\n",
        "d=[]\n",
        "ABCD = []\n",
        "#------------------------------------------------------------------------ASYMMETRY----------------------------------------------------\n",
        "for fil in glob.glob('/content/drive/MyDrive/sample/*.JPG'):\n",
        "    # Load the image\n",
        "    img1 = cv2.imread(fil)\n",
        "    img = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate the center of gravity\n",
        "    M = cv2.moments(img)\n",
        "    cg_x = int(M['m10']/M['m00'])\n",
        "    cg_y = int(M['m01']/M['m00'])\n",
        "\n",
        "    # Draw two orthogonal axes\n",
        "    cv2.line(img, (cg_x - 100, cg_y), (cg_x + 100, cg_y), (0, 0, 255), 2)\n",
        "    cv2.line(img, (cg_x, cg_y - 100), (cg_x, cg_y + 100), (0, 0, 255), 2)\n",
        "\n",
        "    # Divide the lesion into four quadrants\n",
        "    quadrants = [\n",
        "        img[0:cg_y, 0:cg_x],\n",
        "        img[0:cg_y, cg_x:],\n",
        "        img[cg_y:, 0:cg_x],\n",
        "        img[cg_y:, cg_x:]\n",
        "    ]\n",
        "\n",
        "    # Calculate the mean and standard deviation for each quadrant\n",
        "    mean_bright = []\n",
        "    mean_color = []\n",
        "    std_bright = []\n",
        "    std_color = []\n",
        "\n",
        "    for quadrant in quadrants:\n",
        "        if quadrant.ndim == 2:\n",
        "            # Quadrant is grayscale, with only one channel\n",
        "            mean_bright.append(np.mean(quadrant))\n",
        "            mean_color.append(0)\n",
        "            std_bright.append(np.std(quadrant))\n",
        "            std_color.append(0)\n",
        "        else:\n",
        "            # Quadrant is color, with three channels\n",
        "            mean_bright.append(np.mean(quadrant[:,:,0]))\n",
        "            mean_color.append(np.mean(quadrant[:,:,1:], axis=(0,1)))\n",
        "            std_bright.append(np.std(quadrant[:,:,0]))\n",
        "            std_color.append(np.std(quadrant[:,:,1:], axis=(0,1)))\n",
        "\n",
        "    # Calculate the asymmetry score for each axis\n",
        "    axis1_score = abs(mean_bright[0]-mean_bright[2])/max(std_bright[0], std_bright[2])\n",
        "    axis2_score = abs(mean_bright[1]-mean_bright[3])/max(std_bright[1], std_bright[3])\n",
        "    if(axis1_score>0.2 and axis2_score >0.2):\n",
        "      asymmetry_score=2;\n",
        "    elif(axis1_score>0.2 or axis2_score>0.2):\n",
        "      asymmetry_score=1;\n",
        "    else:\n",
        "      asymmetry_score=0;\n",
        "    print(\"Asymmetry score:\", asymmetry_score)\n",
        "    a.append(asymmetry_score)\n",
        "\n"
      ],
      "metadata": {
        "id": "4UYfb68G45Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------BORDER-----------------------------------------------\n",
        "\n",
        "for fil in glob.glob('/content/drive/MyDrive/output_directory3/*.png'):\n",
        "  # Load the image\n",
        "  img = cv2.imread(fil)\n",
        "\n",
        "  # Divide the image into eight equal parts (slices)\n",
        "  slices = []\n",
        "  for i in range(8):\n",
        "    y1 = i * img.shape[0] // 8\n",
        "    y2 = (i+1) * img.shape[0] // 8\n",
        "    slice = img[y1:y2, :]\n",
        "    slices.append(slice)\n",
        "\n",
        "  # Initialize the border score\n",
        "  border_score = 0\n",
        "\n",
        "  # Loop over each slice and calculate the border score\n",
        "  for slice in slices:\n",
        "    # Convert the slice to grayscale\n",
        "    gray = cv2.cvtColor(slice, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply a Canny edge detector to the slice\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "\n",
        "    # Calculate the number of contours in the slice\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    num_contours = len(contours)\n",
        "\n",
        "    # Calculate the border score for this slice\n",
        "    slice_border_score = 1 if num_contours > 1 else 0\n",
        "\n",
        "    # Add the slice border score to the overall border score\n",
        "    border_score += slice_border_score\n",
        "\n",
        "  # Print the overall border score\n",
        "  print('Border score:', border_score)\n",
        "  b.append(border_score)\n",
        ""
      ],
      "metadata": {
        "id": "p9HvKQSz5Gku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "# Define the suspicious colors\n",
        "suspicious_colors = {\n",
        "    \"white\": (255, 255, 255),\n",
        "    \"red\": (0, 0, 255),\n",
        "    \"black\": (0, 0, 0),\n",
        "    \"light_brown\": (165, 123, 63),\n",
        "    \"dark_brown\": (60, 20, 20),\n",
        "    \"blue_gray\": (100, 149, 237)\n",
        "}\n",
        "\n",
        "# Load all the images in the directory\n",
        "for fil in glob.glob('/content/drive/MyDrive/sample/*.JPG'):\n",
        "    # Load the image\n",
        "    img = cv2.imread(fil)\n",
        "\n",
        "    # Convert the image to HSV color space\n",
        "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Initialize the color score to 0\n",
        "    color_score = 0\n",
        "\n",
        "    # Calculate the total number of pixels in the image\n",
        "    num_pixels = img.shape[0] * img.shape[1]\n",
        "\n",
        "    # Check for each suspicious color\n",
        "    for color_name, color_value in suspicious_colors.items():\n",
        "        # Create a mask for the color\n",
        "        lower_range = np.array([color_value[0]-10, 100, 100])\n",
        "        upper_range = np.array([color_value[0]+10, 255, 255])\n",
        "        mask = cv2.inRange(hsv_img, lower_range, upper_range)\n",
        "\n",
        "        # Calculate the number of pixels of the color\n",
        "        num_color_pixels = cv2.countNonZero(mask)\n",
        "\n",
        "        # Check if the color is present in the image\n",
        "        if num_color_pixels >= num_pixels * 0.05:\n",
        "            color_score += 1\n",
        "\n",
        "    print(\"Color score:\", color_score)\n",
        "    c.append(color_score)"
      ],
      "metadata": {
        "id": "6Cx4RmbL5Nzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------DERMOSCOPIC STRUCTURES---------------------------------------\n",
        "for fil in glob.glob('/content/drive/MyDrive/output_directory3/*.png'):\n",
        "    img = cv2.imread(fil)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    ret, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    network = False\n",
        "    structureless = False\n",
        "    branched_streaks = False\n",
        "    dots = False\n",
        "    globules = False\n",
        "\n",
        "    for contour in contours:\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area < 20:\n",
        "            continue\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        aspect_ratio = float(w) / h\n",
        "\n",
        "        if aspect_ratio > 5:\n",
        "            network = True\n",
        "        elif aspect_ratio < 1.5:\n",
        "            if area > 50:\n",
        "                structureless = True\n",
        "            elif area > 20:\n",
        "                dots = True\n",
        "        elif aspect_ratio < 2.5:\n",
        "            if area > 70:\n",
        "                branched_streaks = True\n",
        "        elif aspect_ratio < 5:\n",
        "            if area > 70:\n",
        "                globules = True\n",
        "\n",
        "    score = 0\n",
        "    if network:\n",
        "        score += 1\n",
        "    if structureless:\n",
        "        score += 1\n",
        "    if branched_streaks:\n",
        "        score += 1\n",
        "    if dots:\n",
        "        score += 1\n",
        "    if globules:\n",
        "        score += 1\n",
        "\n",
        "    print(f\"Dermoscopic score: {score}\")\n",
        "    d.append(score)\n"
      ],
      "metadata": {
        "id": "YwBY5WGs5gzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "output_dir1 = '/content/drive/MyDrive/total1/benign'\n",
        "output_dir2 = '/content/drive/MyDrive/total1/Melanoma'\n",
        "if not os.path.exists(output_dir1):\n",
        " os.makedirs(output_dir1)\n",
        "if not os.path.exists(output_dir2):\n",
        " os.makedirs(output_dir2)\n",
        "\n",
        "input_dir = '/content/drive/MyDrive/output_directory3'\n",
        "\n",
        "i=0;\n",
        "for filename in os.listdir(input_dir):\n",
        "   img = cv2.imread(os.path.join(input_dir, filename))\n",
        "   ABCD[i]=float(a[i])*1.3+float(b[i])*0.1+float(c[i])*0.5+float(d[i])*0.5\n",
        "   print(ABCD[i])\n",
        "   if(ABCD[i]>4.45):\n",
        "     cv2.imwrite(os.path.join(output_dir1, filename), img)\n",
        "   else:\n",
        "     cv2.imwrite(os.path.join(output_dir2, filename), img)\n",
        "   i=i+1"
      ],
      "metadata": {
        "id": "EyTqPw9T5uSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "# Set up the directory paths for the two types of images\n",
        "class_1_dir = \"/content/drive/MyDrive/total1/benign\"\n",
        "class_2_dir = \"/content/drive/MyDrive/total1/Melanoma\"\n",
        "# Create a list to store the image file paths and labels\n",
        "data = []\n",
        "# Loop through the images in class 1 directory and add them to the data list with label 0\n",
        "for img_file in os.listdir(class_1_dir):\n",
        "    img_path = os.path.join(class_1_dir, img_file)\n",
        "    data.append((img_path, 0))\n",
        "# Loop through the images in class 2 directory and add them to the data list with label 1\n",
        "for img_file in os.listdir(class_2_dir):\n",
        "    img_path = os.path.join(class_2_dir, img_file)\n",
        "    data.append((img_path, 1))\n",
        "# Shuffle the data to randomize the order of the images\n",
        "import random\n",
        "random.shuffle(data)\n",
        "# Write the data to a CSV file\n",
        "with open(\"binary_classification_dataset.csv\", \"w\", newline=\"\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow([\"image_path\", \"label\"])\n",
        "    for img_path, label in data:\n",
        "        writer.writerow([img_path, label])"
      ],
      "metadata": {
        "id": "aaelC-Q8H3Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras import models\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Load the image data and labels\n",
        "images = np.load('/content/gdrive/MyDrive/images.npy')\n",
        "labels = np.load('/content/gdrive/MyDrive/labels.npy')\n",
        "\n",
        "# Set the image dimensions and batch size\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 16\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
        "# Define the VGG16 model and add additional layers\n",
        "vgg_model = Sequential()\n",
        "\n",
        "pretrained_model = VGG16(include_top=False,\n",
        "                   input_shape=(224, 224, 3),\n",
        "                   pooling='avg',classes=2,\n",
        "                   weights='imagenet')\n",
        "# Unfreeze the layers in the pre-trained model\n",
        "for layer in pretrained_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "vgg_model.add(pretrained_model)\n",
        "vgg_model.add(layers.Flatten())\n",
        "vgg_model.add(layers.Dense(1024, activation='relu'))\n",
        "vgg_model.add(layers.Dropout(0.5))\n",
        "vgg_model.add(layers.Dense(1024, activation='relu'))\n",
        "vgg_model.add(layers.Dropout(0.5))\n",
        "vgg_model.add(layers.Dense(1, kernel_regularizer=l2(0.01), activation='linear'))\n",
        "# Compile the model\n",
        "opt = SGD(learning_rate=0.0001,momentum=0.9)\n",
        "vgg_model.compile(optimizer=opt, loss='hinge', metrics=['accuracy'])\n",
        "\n",
        "# Set the path to save the model in your Google Drive\n",
        "model_path = '/content/gdrive/MyDrive/Models/vggfinetune.h5'\n",
        "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "model_checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True)\n",
        "# Train the model with data augmentation\n",
        "history=vgg_model.fit(X_train, Y_train, batch_size=batch_size, validation_data=(X_test, Y_test), epochs=100, callbacks=[model_checkpoint])"
      ],
      "metadata": {
        "id": "PGwjA-E_VSA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_features = vgg_model.predict(X_train)\n",
        "X_test_features = vgg_model.predict(X_test)\n",
        "print(X_test_features[:10])"
      ],
      "metadata": {
        "id": "ryRF94VjVUdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=[]\n",
        "for i in X_test_features:\n",
        "  if(i>0):\n",
        "    y_pred.append(1);\n",
        "  else:\n",
        "    y_pred.append(0);\n",
        "# Compute the accuracy of the SVM classifier\n",
        "accuracy = accuracy_score(Y_test,y_pred)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "yhDPpyCDVZXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(Y_test, y_pred, target_names=['mel','ben']))\n",
        "\n",
        "print(confusion_matrix(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "d07EKiVRVeNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute the false positive rate (FPR) and true positive rate (TPR) for different thresholds\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, y_pred)\n",
        "\n",
        "# Compute the area under the ROC curve (AUC)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2fvEyiOyViwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.resnet import ResNet101\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Load the dataset\n",
        "data_dir = '/content/gdrive/MyDrive/total1'\n",
        "batch_size = 16\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Define the model\n",
        "base_model = MobileNet(include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
        "x = Dense(1, activation='sigmoid')(base_model.output)\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    verbose=1,\n",
        "    validation_steps=len(val_generator),\n",
        "    steps_per_epoch=len(train_generator)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(val_generator, verbose=1)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "hf9Du_gg5vZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras import models\n",
        "from keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras import models\n",
        "drive.mount('/content/drive')\n",
        "images = np.load('/content/drive/MyDrive/images.npy')\n",
        "labels = np.load('/content/drive/MyDrive/labels.npy')\n",
        "img_height,img_width = 224,224\n",
        "batch_size=40\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(images, labels)):\n",
        "    print(f\"Fold: {fold}\")\n",
        "    X_train, X_val = images[train_idx], images[val_idx]\n",
        "    Y_train, Y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "    resnet_model = Sequential()\n",
        "    pretrained_model= tf.keras.applications.ResNet50(include_top=False,\n",
        "                   input_shape=(224,224,3),\n",
        "                   pooling='avg',classes=2,\n",
        "                   weights='imagenet')\n",
        "    resnet_model.add(pretrained_model)\n",
        "    resnet_model.add(Flatten())\n",
        "    resnet_model.add(Dense(512,activation='relu'))\n",
        "    resnet_model.add(Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='linear'))\n",
        "\n",
        "    for layer in pretrained_model.layers[:-16]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    opt = Adam(learning_rate=0.001)\n",
        "    resnet_model.compile(optimizer = 'adam', loss = 'hinge', metrics = ['accuracy'])\n",
        "\n",
        "    # Set the path to save the model in your Google Drive\n",
        "    model_path = f'/content/drive/MyDrive/Models/new_fold{fold}.h5'\n",
        "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "    model_checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "    history = resnet_model.fit(X_train, Y_train, batch_size=batch_size, validation_data=(X_val, Y_val), epochs=100, callbacks=[model_checkpoint])\n"
      ],
      "metadata": {
        "id": "GfsayZAY6hH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy over epochs\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c3pJ7Bv_Cb5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred=[]\n",
        "for i in X_test_features:\n",
        "  if(i>0):\n",
        "    y_pred.append(1);\n",
        "  else:\n",
        "    y_pred.append(0);\n",
        "# Compute the accuracy of the SVM classifier\n",
        "accuracy = accuracy_score(Y_test,y_pred)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "_2StqwbeA-U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(Y_test, y_pred, target_names=['mel','ben']))\n",
        "\n",
        "print(confusion_matrix(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "4JooKaTe602J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute the false positive rate (FPR) and true positive rate (TPR) for different thresholds\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, y_pred)\n",
        "\n",
        "# Compute the area under the ROC curve (AUC)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9RpJW2FK9GwB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}